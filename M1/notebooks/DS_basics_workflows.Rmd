---
title: 'Data Science Basics: The Data Science Workflow'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output: ioslides_presentation
---

```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  echo = FALSE
  )

# Load packages
library(tidyverse)
library(magrittr)
library(knitr)
library(kableExtra)
```

<!-- CSS Definitions -->
<style type="text/css">
    .img_small{
        width: 50%;
        class="centered";
        display: block;
    }
    .img{
        width: 75%;
        class="centered";
        display: block;
    }
    .img_big{
        width: 100%;
        class="centered";
        display: block;
    }
</style>
<!-- CSS Definitions -->

## This session 

In this session you will be introduced to: 

1. xxx
2. xxx

# Introduction 

## Data Science Workflows {.smaller}

![](https://sds-aau.github.io/SDS-master/00_media/ds_workflow.png){.img}

1. **Import** Transfer data from an external source (xls sheet, html page, API etc.) to a format suitable for your current environment.
2. **Tidy:** Manipulating data structure in a consistent form that matches the semantics of the dataset.
3. **Transform:** Computing/summrizing variables, subsetting, filtering, merging etc. (data wrangling)
4. **EDA / Visualize:** Exploration of the properties of and relationships within the data with visualizations and simple statistics.
5. **Model:** Use of mathematical/computational tools/algorithms to identify properties of data, relationships, mechanisms & predictions
6. **Communicate:** Present results in a format suitable for the targeted audience.



## Data Import
Objective:
* Navigating through the vastness of data available on every given topic, including online and offline sources
* Identifying the datasource (or combination of many) that suits best to answer your (research) question at hand
* Retrieve and consolidate data from various sources to a dataformat suitable for your further workflow

Skills Required:
* Database Management: MySQL, PostgresSQL,MongoDB
* Querying relational and non-relational databases
* Accessing data via Application Programming Interfaces (API)
* Retrieving Unstructured Data: text, videos, audio files, documents
* Distributed Storage (Advanced): Hadoops, Spark/Flink
* Python Packages
    * Pandas (geopandas if working with GIS data)
    * Requests (working with APIs)
    * Beautiful Soup (HTML parsing)
    * Json/Simplejson (parsing JSON data)
    * Pymongo (MongoDB interface)
    * Sqlite (Sqlite in Python, Pandas can access SQL too)
    * H5Py and Pytables (Working with HDF5 - big nummerical data)
    * Pyspark (in case you venture into Spark and distributed computing)
* R packages
    * Dplyr
    * API interfaces (eg. eurostat, rcrunchbase)
    * rvest (web-scraping)
    * sparklyr (Spark backend for R)
    * mongolite (mongoDB backend for R)







# Summary
