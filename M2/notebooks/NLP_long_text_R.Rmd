---
title: 'Natural-language-Processing (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```

```{r}
### Load standardpackages
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr) # For extra-piping operators (eg. %<>%)
```

### This session

In this applied session, you will:

1. Refresh basic string manipulation skills
2. Learn how to tokenize texts and analyze these tokens
3. xxx


* Now, that we have some experience with short texts, let's try out to work with longer texts. 
* We will be analysing a newspaper article as well as a whole (very long) book. 
* We will continue using more of the `tidytext` functionality.
* However, we will also introduce `Spacy`, a high-level DeepLearning based NLP library that will help us to do complex stuff with not too much code 

# Exploring 'Crime and Punishment'

Let's raise the bar with some **Fyodor Dostoevsky**

![](https://i.pinimg.com/564x/bc/eb/9c/bceb9cef99abbed52b940767c9530bbc.jpg)

```{r}
library(tidytext)
```

## Download Data

```{r}
# We first need to get the book text. It can be conveniently retrieved via the gutenbergr library, linking r to the Gutenberg project
library(gutenbergr)
```

```{r}
# check the id of crime and punishment
gutenberg_metadata %>%
  filter(title == "Crime and Punishment")
```

```{r}
text_raw <- gutenberg_download(2554)
```

```{r}
text_raw %>% glimpse()
```

```{r}
# LEts take a look
text_raw %>% head(200)
```

## Preprocessing

* We see the data is read in by line rather than one cell per book/chapter, or paragraph.
* We also see all original line breaks are contained, including empty lines
* The real book starts at line 102
* The parts and chapters are assigned, info we can probably use.
* to create IDs for the text chunks, we could use the linenumber
* Then we could already get rid of empty and chapter lines

```{r}
text <- text_raw %>%
  select(-gutenberg_id) %>%
  slice(-c(1:102)) %>%
  mutate(line_num = row_number(),# create new variable line_num
         part = cumsum(str_detect(text, regex("^PART [\\divxlc]",
                                                  ignore_case = TRUE)))) %>% # create variable part: Crime and Punishment has 7 parts %>%
         group_by(part) %>%
         mutate(chapter = cumsum(str_detect(text, regex("^CHAPTER [\\divxlc]",
                                                          ignore_case = TRUE)))) %>% # create new variable number of Chapter per part %>%
         ungroup() %>%
  filter(text != "" & !str_detect(text, regex("^[PART|CHAPTER]"))) %>%
  mutate(index = 1:n()) %>%
  relocate(index, line_num, part, chapter, text)
  
```

```{r}
text %>% glimpse()
```

Cool!

```{r}
text_tidy <- text %>% unnest_tokens(word, text, token = 'words') %>%
  anti_join(stop_words, by = 'word')
```

```{r}
text_tidy %>% head(10)
```

## First exploration

### Topwords

```{r}
# top 10 words used in Crime and Punishment
text_tidy %>%
  count(word, sort = TRUE) %>%
  slice(1:10) %>%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = word)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Crime and Punishment: Top 10 words used", x = NULL) 
```

Unsurprisingly, the word used more often corresponds to the name of the main character, Raskolnikov. We can also use a word cloud:

```{r}
# People love wordclouds
library(wordcloud)

text_tidy %>%
  count(word) %>%
  with(wordcloud(word, n, 
                 max.words = 50, 
                 color = "blue"))
```

### Sentiment Analysis

* While interesting, word frequency does not tell us much about the emotions/states of mind present in the novel. 
* For this reason, we will go ahead with a sentiment analysis of “Crime and Punishment”

* While sounding very comlpex, sentiment analysis is usually done in a rather simple way.
* There are already predefined sentiment lexica around, linking words ith certain sentiments
* So we do have to only join our word-token with the corresponding sentiments
* The most popular dictionaries available are
   1. "bing": classifies words binary into positive and negative sentiment
   2. “nrc”  has the following emotion categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust; and
   3. “afinn” corresponds to a sentiment score from -5 (very negative) to 5 (very positive). 


```{r}
# You might need to first install the 'textdata' package for some of the lexica
get_sentiments("bing") %>% 
  head(20)
```

Lets calculate them all

```{r}
sentiment_bing <- text_tidy %>%
  inner_join(get_sentiments("bing")) %>%
  count(chapter, index = index %/% 100, sentiment) %>% # index of 100 lines of text
  mutate(lexicon = 'Bing')
  
sentiment_nrc <- text_tidy %>%  
  inner_join(get_sentiments("nrc")) %>%
  count(chapter, index = index %/% 100, sentiment) %>% # index of 100 lines of text
  mutate(lexicon = 'NRC')

sentiment_afinn <- text_tidy %>%  
  select(-line_num, -part) %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = index %/% 100)  %>% # index of 100 lines of text
  summarise(sentiment = sum(value, na.rm = TRUE)) %>%
  mutate(lexicon = 'AFINN')
```


```{r}
# Lets join them all together for plotting
sentiment_all <- sentiment_afinn %>%
  bind_rows(sentiment_bing %>%
              pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
              mutate(sentiment = positive - negative) %>%
              select(index, sentiment, lexicon) ) %>%
    bind_rows(sentiment_nrc %>%
              pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
              mutate(sentiment = positive - negative) %>%
              select(index, sentiment, lexicon) ) 
```

```{r, fig.height=5, fig.width=15}
# crime and punishment - 
sentiment_all %>%
  ggplot(aes(x = index, y = sentiment, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ lexicon) + 
  labs(title = "Sentiment Analysis: “Crime and Punishment",
       subtitle = 'Using the Bing, NRC, AFINN lexicon') 
```


* Since NRS also provides us with options to dive deeper into specific, rather than only positive and negative sentiments.

```{r}
text_tidy %>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE) %>% 
  filter(sentiment %in% c("joy", "sadness")) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  as.data.frame() %>% 
  remove_rownames() %>% 
  column_to_rownames("word") %>% 
  comparison.cloud(colors = c("darkgreen", "grey75"), 
                   max.words = 100,
                   title.size = 1.5)
```

```{r, fig.height=5, fig.width=15}
# crime and punishment - 
sentiment_nrc %>%
  ggplot(aes(x = index, y = sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ chapter) + 
  labs(title = "Sentiment Analysis: “Crime and Punishment",
       subtitle = 'By chapter: Using NRC lexicon') 
```


### Word network

* Obviously, we already have to sprinkle in some networks here
* The easiest way would bee to look t words that frequently occur together.
* We could do that in a more complicated way, but since we already have the line-nubmer, why not start with creating edges between words in the same line?
* `tidytext` had an amazing function `pairwise_count` for that. However, since it is of more general use,  the developers outsourced it into the not-text-specific `widyr` package, which they also maintain.

```{r}
library(widyr)
el_words <- text_tidy %>%
  pairwise_count(word, index, sort = TRUE) %>%
  rename(from = item1, to = item2, weight = n)
```

```{r}
el_words %>% head()
```
```{r}
library(tidygraph)
library(ggraph)
```

```{r}
g <- el_words %>%
  filter(weight >= 10) %>%
  as_tbl_graph(directed = FALSE) %>%
  igraph::simplify() %>% as_tbl_graph() 
```


```{r, fig.width=10, fig.height=10}
set.seed(1337)
g %N>%
#  filter(centrality_degree(weight = weight) > 100) %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(width = weight, edge_alpha = weight)) +
  geom_node_point(aes(size = centrality_degree(weight = weight)), color = "plum4") +
  geom_node_text(aes(label = name,), repel = TRUE) +
  theme_graph() +
  theme(legend.position = 'none') +
  labs(title = 'Co-Word Network Crime and Punishment')
  
```

##  Bigrams

TO COME

## NAmed Entity recognition

TO COME

# Endnotes

### Main reference


* Julia Silge and David Robinson (2020). Text Mining with R: A Tidy Approach, O’Reilly. Online available [here](https://www.tidytextmining.com/)
   * [Chapter 2](https://www.tidytextmining.com/sentiment.html): Introduction to sentiment analysis

### Packages & Ecosystem

* [`tidytext`](https://github.com/juliasilge/tidytext)

further: 
* [`rtweet`](https://github.com/ropensci/rtweet): R interface to the twitter API.

### Suggestions for further study

* DataCamp (!Most courses have somewhat outdated ecosystems)
   * [Introduction to Text Analysis in R](https://learn.datacamp.com/courses/introduction-to-text-analysis-in-r): The basics of text analysis in R      
   * [Introduction to Natural Language Processing in R](https://learn.datacamp.com/courses/introduction-to-natural-language-processing-in-r): Some refresher plus more advanced applications in the end.         
  
### Session Info

```{r}
sessionInfo()
```





